{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Llama Stack Quick Start — MCP Option (Optional)\n",
        "\n",
        "This notebook contains **Option B: MCP tool** only. Use it when the Llama Stack MCP adapter is ready. The main quickstart uses client-side tools only.\n",
        "\n",
        "**Prerequisites:** Same as the main quickstart (Section 1–2: install deps, import libs, define `get_weather` is not needed here). Run the **MCP server** below, then **connect and create the agent** with MCP tools. MCP tools are **invoked by the Llama Stack Server (llama-server)**; the MCP server URL must be reachable from where the server runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Option B: MCP tool\n",
        "\n",
        "Run an MCP server that exposes a weather query tool (same capability as the client-side `get_weather`, via MCP). This example uses **Streamable HTTP** (single `/mcp` endpoint; SSE is deprecated). The server is registered with Llama Stack in the next section. *Requires the Llama Stack Server to have `tool_runtime` with the `model-context-protocol` provider.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the MCP server in a separate process\n",
        "import os\n",
        "from multiprocessing import Process\n",
        "\n",
        "def _run_mcp_weather_server():\n",
        "    import logging\n",
        "    logging.basicConfig(level=logging.DEBUG, format='%(name)s %(levelname)s: %(message)s')\n",
        "    logging.getLogger(\"mcp\").setLevel(logging.DEBUG)\n",
        "    from urllib.parse import quote\n",
        "    import requests\n",
        "    from mcp.server.fastmcp import FastMCP\n",
        "    mcp = FastMCP(\"demo-weather\", host=\"0.0.0.0\", port=8002)\n",
        "    @mcp.tool()\n",
        "    def get_weather_mcp(city: str) -> str:\n",
        "        \"\"\"Get current weather information for a specified city.\n",
        "\n",
        "        Uses the wttr.in free weather API to fetch weather data.\n",
        "\n",
        "        :param city: City name, e.g., Beijing, Shanghai, Paris\n",
        "        :returns: Dictionary containing weather information including city, temperature and humidity\n",
        "        \"\"\"\n",
        "        try:\n",
        "            encoded_city = quote(city)\n",
        "            url = f\"https://wttr.in/{encoded_city}?format=j1\"\n",
        "            r = requests.get(url, timeout=10)\n",
        "            r.raise_for_status()\n",
        "            data = r.json()\n",
        "            cur = data[\"current_condition\"][0]\n",
        "            return f\"City: {city}, Temperature: {cur['temp_C']}°C, Humidity: {cur['humidity']}%\"\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "    # streamable-http: single endpoint; use transport=\"sse\" and /sse if server only supports legacy SSE\n",
        "    mcp.run(transport=\"streamable-http\")\n",
        "\n",
        "mcp_process = Process(target=_run_mcp_weather_server, daemon=True)\n",
        "mcp_process.start()\n",
        "import socket\n",
        "# Prefer env so Llama Stack Server can reach this URL\n",
        "MCP_SERVER_URL = os.getenv(\"MCP_SERVER_URL\")\n",
        "if not MCP_SERVER_URL:\n",
        "    _host = socket.gethostbyname(socket.gethostname())\n",
        "    if _host.startswith(\"127.\"):\n",
        "        _host = os.getenv(\"MCP_SERVER_HOST\", \"127.0.0.1\")\n",
        "    MCP_SERVER_URL = f\"http://{_host}:8002/mcp\"\n",
        "os.environ[\"MCP_SERVER_URL\"] = MCP_SERVER_URL\n",
        "print(f\"✓ MCP server running at {MCP_SERVER_URL} (Streamable HTTP, tool: get_weather_mcp, bind 0.0.0.0:8002)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connect to Server and Create Agent (MCP tools)\n",
        "\n",
        "Register the MCP tool group and create an agent that uses it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_stack_client import LlamaStackClient, Agent\n",
        "from llama_stack_client.lib.agents.event_logger import AgentEventLogger\n",
        "\n",
        "base_url = os.getenv('LLAMA_STACK_URL', 'http://localhost:8321')\n",
        "client = LlamaStackClient(base_url=base_url)\n",
        "\n",
        "models = client.models.list()\n",
        "llm_model = next(\n",
        "    (m for m in models\n",
        "        if m.custom_metadata and m.custom_metadata.get('model_type') == 'llm'),\n",
        "    None\n",
        ")\n",
        "if not llm_model:\n",
        "    raise Exception('No LLM model found')\n",
        "model_id = llm_model.id\n",
        "\n",
        "MCP_TOOLGROUP_ID = \"mcp::demo-weather\"\n",
        "mcp_server_url = os.getenv(\"MCP_SERVER_URL\", \"http://127.0.0.1:8002/mcp\")\n",
        "client.toolgroups.register(\n",
        "    toolgroup_id=MCP_TOOLGROUP_ID,\n",
        "    provider_id=\"model-context-protocol\",\n",
        "    mcp_endpoint={\"uri\": mcp_server_url},\n",
        ")\n",
        "agent_tools = [{\"type\": \"mcp\", \"server_label\": MCP_TOOLGROUP_ID, \"server_url\": mcp_server_url}]\n",
        "\n",
        "agent = Agent(\n",
        "    client,\n",
        "    model=model_id,\n",
        "    instructions='You are a helpful weather assistant. When users ask about weather, use the weather tool to query and answer.',\n",
        "    tools=agent_tools,\n",
        ")\n",
        "print('Agent created with MCP weather tool')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Troubleshooting (MCP / 400 error)\n",
        "\n",
        "If you see **400 - messages[3]: invalid type: sequence, expected a string**: the inference backend often expects message `content` to be a string, but the server may send tool-turn content as an array. This is a message-format compatibility issue between the server and the backend, **not caused by SSE/Streamable HTTP**. You can:\n",
        "- Use the main quickstart with **client-side tool** (Option A) instead, or\n",
        "- Use **stdio** for MCP (configure the server's `tool_runtime` with `command`/`args` so the server spawns the MCP process; no HTTP URL needed), or\n",
        "- Check your Llama Stack Server and inference backend docs for tool message format compatibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stop the MCP server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'mcp_process' in globals() and mcp_process.is_alive():\n",
        "    mcp_process.terminate()\n",
        "    mcp_process.join(timeout=2)\n",
        "    print(\"✓ MCP server process stopped.\")\n",
        "else:\n",
        "    print(\"MCP server process is not running or has already stopped.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (llama-stack-demo)",
      "language": "python",
      "name": "llama-stack-demo"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
