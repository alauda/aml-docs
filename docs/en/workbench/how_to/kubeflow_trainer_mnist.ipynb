{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# PyTorch DDP Fashion MNIST Training Example\n",
    "\n",
    "This example demonstrates how to train a convolutional neural network (CNN) to classify images using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset and [PyTorch Distributed Data Parallel (DDP)](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html).\n",
    "\n",
    "This notebook walks you through running that example locally, and how to easily scale PyTorch DDP across multiple nodes with Kubeflow TrainJob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install the Kubeflow SDK\n",
    "\n",
    "You need to install the Kubeflow SDK to interact with Kubeflow Trainer APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /tmp/tmpn77klvtj\n",
      "Requirement already satisfied: setuptools in /.venv/lib/python3.11/site-packages (80.9.0)\n",
      "Requirement already satisfied: pip in /.venv/lib/python3.11/site-packages (24.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "kubeflow                           0.3.0\n",
      "kubeflow_katib_api                 0.19.0\n",
      "kubeflow_trainer_api               2.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m ensurepip\n",
    "!python -m pip install -U -q kubeflow -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!python -m pip list | grep kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the PyTorch Dependencies\n",
    "\n",
    "You also need to install PyTorch and Torchvision to be able to run the example locally (need to install g++/gcc in the notebook environment and use GPU optionally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==2.9.1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "#!pip install torchvision==0.22.1 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training Function\n",
    "\n",
    "The first step is to create function to train CNN model using Fashion MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fashion_mnist():\n",
    "    import os\n",
    "\n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    import torch.nn.functional as F\n",
    "    from torch import nn\n",
    "    from torch.utils.data import DataLoader, DistributedSampler\n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "    # Define the PyTorch CNN model to be trained\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "            self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "            self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "            self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.max_pool2d(x, 2, 2)\n",
    "            x = x.view(-1, 4 * 4 * 50)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    # Use NCCL if a GPU is available, otherwise use Gloo as communication backend.\n",
    "    device, backend = (\"cuda\", \"nccl\") if torch.cuda.is_available() else (\"cpu\", \"gloo\")\n",
    "    print(f\"Using Device: {device}, Backend: {backend}\")\n",
    "\n",
    "    # Setup PyTorch distributed.\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", 0))\n",
    "    dist.init_process_group(backend=backend)\n",
    "    print(\n",
    "        \"Distributed Training for WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}\".format(\n",
    "            dist.get_world_size(),\n",
    "            dist.get_rank(),\n",
    "            local_rank,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the model and load it into the device.\n",
    "    device = torch.device(f\"{device}:{local_rank}\")\n",
    "    model =Net().to(device)\n",
    "    model=torch.compile(model)\n",
    "    model = nn.parallel.DistributedDataParallel(model)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    \n",
    "    \n",
    "    # Download FashionMNIST dataset only on local_rank=0 process.\n",
    "    if local_rank == 0:\n",
    "        dataset = datasets.FashionMNIST(\n",
    "            \"./data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor()]),\n",
    "        )\n",
    "    dist.barrier()\n",
    "    dataset = datasets.FashionMNIST(\n",
    "        \"./data\",\n",
    "        train=True,\n",
    "        download=False,\n",
    "        transform=transforms.Compose([transforms.ToTensor()]),\n",
    "    )\n",
    "\n",
    "\n",
    "    # Shard the dataset accross workers.\n",
    "    train_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=100,\n",
    "        sampler=DistributedSampler(dataset)\n",
    "    )\n",
    "\n",
    "    # TODO(astefanutti): add parameters to the training function\n",
    "    dist.barrier()\n",
    "    for epoch in range(1, 3):\n",
    "        model.train()\n",
    "\n",
    "        # Iterate over mini-batches from the training set\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Copy the data to the GPU device if available\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = F.nll_loss(outputs, labels)\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 10 == 0 and dist.get_rank() == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(inputs),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Wait for the distributed training to complete\n",
    "    dist.barrier()\n",
    "    if dist.get_rank() == 0:\n",
    "        print(\"Training is finished\")\n",
    "\n",
    "    # Finally clean up PyTorch distributed\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Training Locally (Optional)\n",
    "\n",
    "We can submit the training function to the local Trainer client to run it in an isolated subprocess.\n",
    "\n",
    "> **Note: skip below code block if you do not have g++/gcc and GPU in your notebook instance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow.trainer import CustomTrainer, TrainerClient, LocalProcessBackendConfig\n",
    "\n",
    "# Initialize local backend\n",
    "backend_config = LocalProcessBackendConfig(cleanup_venv=True)\n",
    "client = TrainerClient(backend_config=backend_config)\n",
    "\n",
    "# List available runtimes\n",
    "for runtime in client.list_runtimes():\n",
    "    if runtime.name == \"torch-distributed\":\n",
    "        torch_runtime = runtime\n",
    "        break\n",
    "\n",
    "# Submit training job\n",
    "job_name = client.train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=train_fashion_mnist,\n",
    "        packages_to_install=[\"torch\", \"torchvision\"],\n",
    "        pip_index_urls=[\"https://pypi.tuna.tsinghua.edu.cn/simple\"],\n",
    "    ),\n",
    "    runtime=torch_runtime,\n",
    ")\n",
    "\n",
    "# Stream logs\n",
    "for logline in client.get_job_logs(job_name, follow=True):\n",
    "    print(logline, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale PyTorch DDP with Kubeflow TrainJob\n",
    "\n",
    "You can use `TrainerClient()` from the Kubeflow SDK to communicate with Kubeflow Trainer APIs and scale your training function across multiple PyTorch training nodes.\n",
    "\n",
    "`TrainerClient()` verifies that you have required access to the Kubernetes cluster.\n",
    "\n",
    "Kubeflow Trainer creates a `TrainJob` resource and automatically sets the appropriate environment variables to set up PyTorch in distributed environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from kubeflow.trainer import CustomTrainer, TrainerClient\n",
    "\n",
    "client = TrainerClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the Training Runtimes\n",
    "\n",
    "You can get the list of available Training Runtimes to start your TrainJob.\n",
    "\n",
    "Additionally, it might show available accelerator type and number of available resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime(name='torch-distributed', trainer=RuntimeTrainer(trainer_type=<TrainerType.CUSTOM_TRAINER: 'CustomTrainer'>, framework='torch', image='152-231-registry.alauda.cn:60070/mlops/torch-distributed:v2.9.1-aml2', num_nodes=1, device='Unknown', device_count='Unknown'), pretrained_model=None)\n"
     ]
    }
   ],
   "source": [
    "for runtime in client.list_runtimes():\n",
    "    print(runtime)\n",
    "    if runtime.name == \"torch-distributed\":\n",
    "        torch_runtime = runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Distributed TrainJob\n",
    "\n",
    "Kubeflow TrainJob will train the above model on 3 PyTorch nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = client.train(\n",
    "    trainer=CustomTrainer(\n",
    "        func=train_fashion_mnist,\n",
    "        # Set how many PyTorch nodes you want to use for distributed training.\n",
    "        num_nodes=1,\n",
    "        # Set the resources for each PyTorch node.\n",
    "        resources_per_node={\n",
    "            \"cpu\": 2,\n",
    "            \"memory\": \"8Gi\",\n",
    "            # Uncomment this to distribute the TrainJob using GPU nodes.\n",
    "            # \"nvidia.com/gpu\": 1,\n",
    "        },\n",
    "    ),\n",
    "    runtime=torch_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the TrainJob steps\n",
    "\n",
    "You can check the components of TrainJob that's created.\n",
    "\n",
    "Since the TrainJob performs distributed training across 3 nodes, it generates 3 steps: `trainer-node-0` .. `trainer-node-2`.\n",
    "\n",
    "You can get the individual status for each of these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the running status.\n",
    "client.wait_for_job_status(name=job_name, status={\"Running\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:20:24.045774Z",
     "iopub.status.busy": "2025-09-03T13:20:24.045480Z",
     "iopub.status.idle": "2025-09-03T13:20:24.772877Z",
     "shell.execute_reply": "2025-09-03T13:20:24.772178Z",
     "shell.execute_reply.started": "2025-09-03T13:20:24.045755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: node-0, Status: Running, Devices: gpu x 1\n",
      "\n",
      "Step: node-1, Status: Running, Devices: gpu x 1\n",
      "\n",
      "Step: node-2, Status: Running, Devices: gpu x 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in client.get_job(name=job_name).steps:\n",
    "    print(f\"Step: {c.name}, Status: {c.status}, Devices: {c.device} x {c.device_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the TrainJob logs\n",
    "\n",
    "We can use the `get_job_logs()` API to get the TrainJob logs.\n",
    "\n",
    "Since we run training on 3 GPUs, every PyTorch node uses 60,000/3 = 20,000 images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T13:20:26.729486Z",
     "iopub.status.busy": "2025-09-03T13:20:26.728951Z",
     "iopub.status.idle": "2025-09-03T13:20:29.596510Z",
     "shell.execute_reply": "2025-09-03T13:20:29.594741Z",
     "shell.execute_reply.started": "2025-09-03T13:20:26.729446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W903 13:20:06.250970596 socket.cpp:755] [c10d] The IPv6 network addresses of (j819fe212f8e-node-0-0.j819fe212f8e, 29500) cannot be retrieved (gai error: -2 - Name or service not known).\n",
      "[W903 13:20:07.936850072 socket.cpp:755] [c10d] The IPv6 network addresses of (j819fe212f8e-node-0-0.j819fe212f8e, 29500) cannot be retrieved (gai error: -2 - Name or service not known).\n",
      "[W903 13:20:08.873279869 socket.cpp:755] [c10d] The IPv6 network addresses of (j819fe212f8e-node-0-0.j819fe212f8e, 29500) cannot be retrieved (gai error: -2 - Name or service not known).\n",
      "[W903 13:20:08.673190939 socket.cpp:755] [c10d] The IPv6 network addresses of (j819fe212f8e-node-0-0.j819fe212f8e, 29500) cannot be retrieved (gai error: -2 - Name or service not known).\n",
      "[W903 13:20:09.683786251 socket.cpp:755] [c10d] The IPv6 network addresses of (j819fe212f8e-node-0-0.j819fe212f8e, 29500) cannot be retrieved (gai error: -2 - Name or service not known).\n",
      "Using Device: cuda, Backend: nccl\n",
      "Distributed Training for WORLD_SIZE: 3, RANK: 0, LOCAL_RANK: 0\n",
      "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.2MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 213kB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.85MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 43.7MB/s]\n",
      "/opt/conda/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
      "  warnings.warn(  # warn only once\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.297219\n",
      "Train Epoch: 1 [1000/60000 (5%)]\tLoss: 1.803301\n",
      "Train Epoch: 1 [2000/60000 (10%)]\tLoss: 1.663171\n",
      "Train Epoch: 1 [3000/60000 (15%)]\tLoss: 1.462810\n",
      "Train Epoch: 1 [4000/60000 (20%)]\tLoss: 1.270390\n",
      "Train Epoch: 1 [5000/60000 (25%)]\tLoss: 0.977784\n",
      "Train Epoch: 1 [6000/60000 (30%)]\tLoss: 0.791962\n",
      "Train Epoch: 1 [7000/60000 (35%)]\tLoss: 0.626636\n",
      "Train Epoch: 1 [8000/60000 (40%)]\tLoss: 0.613735\n",
      "Train Epoch: 1 [9000/60000 (45%)]\tLoss: 0.590524\n",
      "Train Epoch: 1 [10000/60000 (50%)]\tLoss: 0.529664\n",
      "Train Epoch: 1 [11000/60000 (55%)]\tLoss: 0.448692\n",
      "Train Epoch: 1 [12000/60000 (60%)]\tLoss: 0.508065\n",
      "Train Epoch: 1 [13000/60000 (65%)]\tLoss: 0.519867\n",
      "Train Epoch: 1 [14000/60000 (70%)]\tLoss: 0.485550\n",
      "Train Epoch: 1 [15000/60000 (75%)]\tLoss: 0.377857\n",
      "Train Epoch: 1 [16000/60000 (80%)]\tLoss: 0.456072\n",
      "Train Epoch: 1 [17000/60000 (85%)]\tLoss: 0.563855\n",
      "Train Epoch: 1 [18000/60000 (90%)]\tLoss: 0.426923\n",
      "Train Epoch: 1 [19000/60000 (95%)]\tLoss: 0.379981\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.416116\n",
      "Train Epoch: 2 [1000/60000 (5%)]\tLoss: 0.506344\n",
      "Train Epoch: 2 [2000/60000 (10%)]\tLoss: 0.510181\n",
      "Train Epoch: 2 [3000/60000 (15%)]\tLoss: 0.388432\n",
      "Train Epoch: 2 [4000/60000 (20%)]\tLoss: 0.361425\n",
      "Train Epoch: 2 [5000/60000 (25%)]\tLoss: 0.428971\n",
      "Train Epoch: 2 [6000/60000 (30%)]\tLoss: 0.409087\n",
      "Train Epoch: 2 [7000/60000 (35%)]\tLoss: 0.423781\n",
      "Train Epoch: 2 [8000/60000 (40%)]\tLoss: 0.530143\n",
      "Train Epoch: 2 [9000/60000 (45%)]\tLoss: 0.390188\n",
      "Train Epoch: 2 [10000/60000 (50%)]\tLoss: 0.359034\n",
      "Train Epoch: 2 [11000/60000 (55%)]\tLoss: 0.353203\n",
      "Train Epoch: 2 [12000/60000 (60%)]\tLoss: 0.394956\n",
      "Train Epoch: 2 [13000/60000 (65%)]\tLoss: 0.407927\n",
      "Train Epoch: 2 [14000/60000 (70%)]\tLoss: 0.296730\n",
      "Train Epoch: 2 [15000/60000 (75%)]\tLoss: 0.318915\n",
      "Train Epoch: 2 [16000/60000 (80%)]\tLoss: 0.317562\n",
      "Train Epoch: 2 [17000/60000 (85%)]\tLoss: 0.396108\n",
      "Train Epoch: 2 [18000/60000 (90%)]\tLoss: 0.302800\n",
      "Train Epoch: 2 [19000/60000 (95%)]\tLoss: 0.364170\n",
      "Training is finished\n"
     ]
    }
   ],
   "source": [
    "for logline in client.get_job_logs(job_name, follow=True):\n",
    "    print(logline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the TrainJob\n",
    "\n",
    "When TrainJob is finished, you can delete the resource.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.delete_job(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
