---
weight: 10
---

# Upgrade from AML 1.2
Remember that Install Alauda AI Essentials must be executed as the last step

## Install Alauda AI Cluster Components

Please visit [Alauda AI Cluster](../installation/ai-cluster.mdx) for:

:::warning
Please ignore `Creating Alauda AI Cluster Instance` since we are upgrading from **AML** `1.2`.
:::

1. [Downloading](../installation/ai-cluster.mdx#downloading) operator bundle packages for `Alauda AI Cluster` and `KServeless`.
2. [Uploading](../installation/ai-cluster.mdx#uploading) operator bundle packages to the destination cluster.
3. [Install the Alauda AI Cluster Operator](../installation/ai-cluster.mdx#install-the-alauda-ai-cluster-operator) to the destination cluster.

## Upgrading

The following procedure describes how to upgrade from **AML** `1.2` to **Alauda AI** `1.3`.

Please ensure `aml-operator` pod is up and running in`aml-operator` namespace before upgrading:

```bash
kubectl -naml-operator get pod
```

<Steps>
### Migrate profile resources
Due to the breaking change of Manged ns, you need to migrate the old profiles resources to the new crd of "amlnamespaces". Run the following script in the business cluster where aml is deployed.
```bash
#!/bin/bash

profiles=$(kubectl get profiles -o jsonpath='{.items[*].metadata.name}')

for profile in $profiles; do
  if kubectl get amlnamespace "$profile" &>/dev/null; then
    echo "⚠️  AmlNamespace $profile already exists. Skipping..."
    continue
  fi

  profile_yaml=$(kubectl get profile "$profile" -o yaml)

  build_registry_file=$(mktemp)
  from_registry_file=$(mktemp)
  s3_file=$(mktemp)

  echo "$profile_yaml" | yq '.spec.plugins[] | select(.kind == "AmlConfig") | .spec.buildRegistry' > "$build_registry_file"
  echo "$profile_yaml" | yq '.spec.plugins[] | select(.kind == "AmlConfig") | .spec.fromRegistry' > "$from_registry_file"
  echo "$profile_yaml" | yq '.spec.plugins[] | select(.kind == "AmlConfig") | .spec.s3' > "$s3_file"

  amlnamespace_yaml=$(cat <<EOF
apiVersion: manage.aml.dev/v1alpha1
kind: AmlNamespace
metadata:
  name: $profile
spec:
  config: {}
EOF
)

  if [[ -s "$build_registry_file" && $(yq 'type' "$build_registry_file") != "null" ]]; then
    amlnamespace_yaml=$(echo "$amlnamespace_yaml" | yq ".spec.config.buildRegistry = load(\"$build_registry_file\")")
  fi

  if [[ -s "$from_registry_file" && $(yq 'type' "$from_registry_file") != "null" ]]; then
    amlnamespace_yaml=$(echo "$amlnamespace_yaml" | yq ".spec.config.fromRegistry = load(\"$from_registry_file\")")
  fi

  if [[ -s "$s3_file" && $(yq 'type' "$s3_file") != "null" ]]; then
    amlnamespace_yaml=$(echo "$amlnamespace_yaml" | yq ".spec.config.s3 = load(\"$s3_file\")")
  fi

  rm -f "$build_registry_file" "$from_registry_file" "$s3_file"

  echo "$amlnamespace_yaml" | kubectl apply -f -
  echo "✅ AmlNamespace $profile created."
done
```

### Migrate inferenceservices resources

Due to the upgrade of kserve. The container name verification of the inference service is more stringent and must be kserve-container. Therefore, the old data needs to be migrated for normal use.
Run the following script in the business cluster where aml is deployed.
```bash
#!/bin/bash

set -e

# 获取所有带有 app.kubernetes.io/part-of: kubeflow-profile 标签的命名空间
NAMESPACES=$(kubectl get ns -l app.kubernetes.io/part-of=kubeflow-profile -o jsonpath='{.items[*].metadata.name}')

for ns in $NAMESPACES; do
    echo "Processing namespace: $ns"

    # 获取当前 namespace 下所有 InferenceService 资源的名称
    SERVICES=$(kubectl get inferenceservice -n "$ns" -o jsonpath='{.items[*].metadata.name}')

    for svc in $SERVICES; do
        echo "Checking InferenceService: $svc in namespace: $ns"

        # 获取当前 spec.predictor.model.name
        MODEL_NAME=$(kubectl get inferenceservice "$svc" -n "$ns" -o jsonpath='{.spec.predictor.model.name}')

        if [ "$MODEL_NAME" != "kserve-container" ]; then
            echo "Updating $svc in namespace: $ns (model.name: $MODEL_NAME -> kserve-container)"

            # 使用 kubectl patch 进行修改
            kubectl patch inferenceservice "$svc" -n "$ns" --type=json -p='[{"op": "replace", "path": "/spec/predictor/model/name", "value": "kserve-container"}]'
        else
            echo "No change needed for $svc in namespace: $ns"
        fi
    done
done

echo "Script execution completed."
```

### Migrate Knative CRD

Due to the breaking change of Knative CRD, you need to migrate the CRD to the new version before upgrading.

```bash
kubectl exec -it deploy/aml-operator -- /app/storageversion-migrate \
    services.serving.knative.dev \
    configurations.serving.knative.dev \
    revisions.serving.knative.dev \
    routes.serving.knative.dev \
    domainmappings.serving.knative.dev
```

Wait for the command to complete, the command output should like this:

```
INFO    storageversion/main.go:61       Migrating group resources       {"len": 5}
INFO    storageversion/main.go:64       Migrating group resource        {"resource": "services.serving.knative.dev"}
INFO    storageversion/main.go:64       Migrating group resource        {"resource": "configurations.serving.knative.dev"}
INFO    storageversion/main.go:64       Migrating group resource        {"resource": "revisions.serving.knative.dev"}
INFO    storageversion/main.go:64       Migrating group resource        {"resource": "routes.serving.knative.dev"}
INFO    storageversion/main.go:64       Migrating group resource        {"resource": "domainmappings.serving.knative.dev"}
INFO    storageversion/main.go:74       Migration complete
```

### Remove resources blocks upgrading

Some resources block upgrading so manual action is required.

Execute following command to remove problematic resources:

```bash
kubectl -nknative-serving delete pdb activator-pdb webhook-pdb
```

### Retrieve AML 1.2 Values

:::info
`helm` and `yq` utilities should be installed on the cluster node if not present.
:::

In the AML cluster, you can retrieve the values of AML 1.2 with the following `helm` command:

```bash
helm get values aml -nkubeflow -oyaml  | tee /tmp/aml-values.yaml
```

Please save the generated `/tmp/aml-values.yaml` file for later use.

### Creating GitLab Admin Token Secret

We need to create a secret for GitLab admin token, which is required to run **Alauda AI Cluster**.

Please follow instructions described in [Pre-Configuration](../installation//pre-configuration.mdx#gitlab-configuration).

### Creating MySQL Secret

We also need to create a MySQL secret for **Alauda AI Cluster**.

Run the following command:

```bash
MYSQL_PASSWORD=$(kubectl -nkubeflow get secret mysql-secret -o jsonpath='{.data.password}' | base64 -d)  # [!code callout]

# [!code callout:3]
kubectl create secret generic aml-mysql-secret \
  --from-literal="password=${MYSQL_PASSWORD}" \
  -n cpaas-system
```

<Callouts>

1. Retrieve the MySQL password from previous AML 1.2 installed secret resource.
2. Create a MySQL admin token secret named **aml-mysql-secret**
3. The password is saved under **password** key.
4. The secret is created under **cpaas-system** namespace.

</Callouts>

### Creating Alauda AI Cluster Instance

Finally we create an `AmlCluster` resource named `default` for **Alauda AI Cluster** to finish the upgrading.

Create a yaml file named `aml-cluster.yaml` with the following content:

```yaml title="aml-cluster.yaml"
apiVersion: amlclusters.aml.dev/v1alpha1
kind: AmlCluster
metadata:
  name: default
spec:
  values:
    global:
      # single-node or ha-cluster
      deployFlavor: single-node
      gitlabBaseUrl: "${GITLAB_BASE_URL}"
      gitlabAdminTokenSecretRef:  # [!code callout]
        name: aml-gitlab-admin-token
        namespace: cpaas-system
      mysql:
        host: "${MYSQL_HOST}"
        port: ${MYSQL_PORT}
        user: "${MYSQL_USER}"
        database: aml
        passwordSecretRef:  # [!code callout]
          name: aml-mysql-secret
          namespace: cpaas-system
    buildkitd:
      storage:
        type: emptyDir
  components:
    kserve:
      managementState: Managed  # [!code callout]
    knativeServing:
      managementState: Managed
      ingressGateway:
        domain: "*.example.com"  # [!code callout]
        certificate:
          secretName: knative-serving-cert
          type: SelfSigned
```

<Callouts>

1. The `name` and `namespace` of GitLab admin token secret created previously.
2. The `name` and `namespace` of MySQL secret created previously.
3. Set the management state of `kserve` to `Managed`, which means the `KServe` will be installed and managed by **Alauda AI Cluster**.
4. Set the management state of `knativeServing` to `Managed`, which means the `Knative Serving` will be installed and managed by **Alauda AI Cluster**.
5. The wildcard `domain` for exposing inference services.

  :::info
  The `domain` and `certificate` are required for exposing inference services and reserved fo future use.
  :::

</Callouts>

Executing the following commands to retrieve necessary context from **AML** `1.2`, and then install **Alauda AI Cluster**:

```bash
# Retrieve the GitLab host scheme and base from the AML 1.2 values file.
GITLAB_HOST_SCHEME=$(yq -r .global.gitScheme < /tmp/aml-values.yaml)
GITLAB_HOST_BASE=$(yq -r .global.gitBase < /tmp/aml-values.yaml)
export GITLAB_BASE_URL="${GITLAB_HOST_SCHEME}://${GITLAB_HOST_BASE}"

# Retrieve the MySQL host, port and user from the AML 1.2 values file.
export MYSQL_HOST=$(yq -r .global.mysqlHost < /tmp/aml-values.yaml)
export MYSQL_PORT=$(yq -r .global.mysqlPort < /tmp/aml-values.yaml)
export MYSQL_USER=$(yq -r .global.mysqlUsername < /tmp/aml-values.yaml)

# [!code callout:2]
yq '(.. | select(tag == "!!str")) |= envsubst' aml-cluster.yaml \
  | kubectl apply -f -
```

<Callouts>

1. Render environment variables for `aml-cluster.yaml` file.
2. Create the `AmlCluster` resource to install **Alauda AI Cluster**.

</Callouts>

</Steps>

## Verification

Check the status field from the `AMLCluster` resource which named `default`:

```bash
kubectl get amlcluster default
```

Should returns `Ready`:

```
NAME      READY   REASON
default   True    Succeeded
```

## Install Alauda AI Essentials

Please visit [Alauda AI Essentials](../installation/ai-essentials.mdx) for installation instructions.
