---
weight: 10
i18n:
  title:
    en: Extend LLM Inference Runtimes
    zh: 扩展 AI 平台 LLM 推理服务运行时
---

# Extend LLM Inference Runtimes

## Introduction

This document will guide you step-by-step on how to add new inference runtimes for Large Language Model (LLM) inference services within the AI Platform. By introducing custom runtimes, you can expand the platform's support for a wider range of model types and domestic chip hardware, and optimize performance for specific scenarios to meet broader business needs.

## Scenarios

Consider extending your AI Platform LLM inference service runtimes if you encounter any of the following situations:

* **Support for New Model Types**: Your LLM model isn't natively supported by the current default inference runtimes (e.g., vLLM or Seldon MLServer).
* **Compatibility with Domestic Chips**: You need to perform LLM inference on hardware equipped with domestic chips, either to leverage their specific advantages or to meet localization requirements.
* **Performance Optimization for Specific Scenarios**: In certain inference scenarios, a new runtime (like Xinference) might offer better performance or resource utilization compared to existing runtimes.
* **Custom Inference Logic**: You need to introduce custom inference logic or dependent libraries that are difficult to implement within the existing default runtimes.


## Prerequisites

Before you start, please ensure you meet these conditions:

1.  Your AI Platform version is **1.3 or higher**.
2.  Your Kubernetes cluster is deployed and **running normally**.
3.  You have the necessary **inference runtime image(s)** prepared. For example, for the Xinference runtime, images might look like `build-harbor.alauda.cn/mlops/xinference:1.2.2-cu121-v1.3.0` (for GPU) or `build-harbor.alauda.cn/mlops/xinference:1.2.2-cpu-v1.3.0` (for CPU).
4.  You have **cluster administrator privileges** (needed to create CRD instances).


## Steps

<Steps>

### Create Inference Runtime Resources

You'll need to create the corresponding inference runtime resources based on your target hardware environment (GPU/CPU/NPU).

1. **Prepare the Runtime YAML Configuration**:

    Based on the type of runtime you want to add and your target hardware environment, prepare the appropriate YAML configuration file. See the **Configuration Examples for Runtimes** section below for sample YAML configurations.

      
2.  **Apply the YAML File to Create the Resource**:

    From a terminal with cluster administrator privileges, execute the following command to apply your YAML file and create the inference runtime resource:
    ```bash
    kubectl apply -f your-runtime.yaml
    ```
    :::tip
    * **Important Tip**: Please **refer to the examples below and create/configure the runtime based on your actual environment and inference needs.** These examples are for reference only. You'll need to adjust parameters like the image, resource `limits`, and `requests` to ensure the runtime is compatible with your model and hardware environment and runs efficiently.
    * **Note**: You can only use this custom runtime on the inference service publishing page *after* the runtime resource has been created!
    :::

### Publish Inference Service and Select the Runtime

Once the custom inference runtime resource is successfully created, you can select and configure it when publishing your LLM inference service on the AI Platform.

1.  **Configure Inference Framework for the Model**:

    Ensure that on the model details page of the model repository you are about to publish, you have selected the appropriate **framework** through the **File Management** metadata editing function. The framework parameter value chosen here must match a value included in the `supportedModelFormats` field when you created the inference service runtime. Please **ensure the model framework parameter value is listed in the `supportedModelFormats` list** set in the inference runtime.
2.  **Navigate to the Inference Service Publishing Page**:

    Log in to the AI Platform and navigate to the "Inference Services" or "Model Deployment" modules, then click "Publish Inference Service."
3.  **Select the Custom Runtime**:

    In the inference service creation wizard, find the "Runtime" or "Inference Framework" option. From the dropdown menu or list, select the custom runtime you created in Step 1 (e.g., "MLServer Runtime" or "Xinference GPU Runtime (CUDA)").
4.  **Set Environment Variables (if needed)**:

    Some runtimes, like Xinference, require specific environment variables to function correctly. On the inference service configuration page, locate the "Environment Variables" or "More Settings" section and add any required environment variables.

</Steps>


## Configuration Examples for Runtimes

### MLServer 

The MLServer runtime is versatile and can be used on both NVIDIA GPUs and CPUs.
  ```yaml
  apiVersion: serving.kserve.io/v1alpha1
  kind: ClusterServingRuntime
  metadata:
    name: aml-mlserver-cuda-11.6
    labels:
      cpaas.io/runtime-class: mlserver
      cpaas.io/accelerator-type: nvidia
      cpaas.io/cuda-version: "11.6"
    annotations:
      cpaas.io/display-name: mlserver-cuda11.6-x86-arm
  spec:
    labels:
      modelClass: mlserver_sklearn.SKLearnModel
    containers:
      - command:
          - /bin/bash
          - -lc
          - |
            mlserver start $MLSERVER_MODEL_URI $@
          # Add this line to use $@ in the script:
          # see: https://unix.stackexchange.com/questions/144514/add-arguments-to-bash-c
          - bash
        env:
          - name: MLSERVER_MODEL_URI
            value: /mnt/models
          - name: MLSERVER_MODEL_NAME
            value: '{{.Name}}'
        image: build-harbor.alauda.cn/mlops/seldon-mlserver:1.6.0-cu116-v1.3.1
        name: kserve-container
        resources:
          limits:
            cpu: 2
            memory: 6Gi
          requests:
            cpu: 2
            memory: 6Gi
        startupProbe:
          httpGet:
            path: /v2/models/{{.Name}}/ready
            port: 8080
            scheme: HTTP
          failureThreshold: 60
          periodSeconds: 10
          timeoutSeconds: 10
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          privileged: false
          runAsNonRoot: true
          runAsUser: 1000
    supportedModelFormats:
      - name: mlflow
        version: "1"
      - name: transformers
        version: "1"

  ```

### Xinference

<Tabs>
<Tab label="GPU">

  ```yaml
  apiVersion: serving.kserve.io/v1alpha1
  kind: ClusterServingRuntime
  metadata:
    name: aml-xinference-cuda-12.1
    labels:
      cpaas.io/runtime-class: xinference # required runtime type label
      cpaas.io/accelerator-type: "nvidia"
      cpaas.io/cuda-version: "12.1"
    annotations:
      cpaas.io/display-name: xinference-cuda-12.1 # Display name in the UI
  spec:
    containers:
      - name: kserve-container
        image: build-harbor.alauda.cn/mlops/xinference:1.2.2-cu121-v1.3.0
        env:
        - name: MODEL_PATH
          value: /mnt/models/{{ index .Annotations "aml-model-repo" }}
        - name: MODEL_UID
          value: '{{ index .Annotations "aml-model-repo" }}'
        - name: MODEL_ENGINE
          value: "transformers"
        # Required parameter for xinference runtime, please set it based on your model family, value: "llama" # e.g., "llama", "chatglm", etc.
        - name: MODEL_FAMILY
          value: ""
        command:
        - bash
        - -lc
        - |
          set +e
          if [ "${MODEL_PATH}" == "" ]; then
              echo "Need to set MODEL_PATH!"
              exit 1
          fi
          if [ "${MODEL_ENGINE}" == "" ]; then
              echo "Need to set MODEL_ENGINE!"
              exit 1
          fi
          if [ "${MODEL_UID}" == "" ]; then
              echo "Need to set MODEL_UID!"
              exit 1
          fi
          if [ "${MODEL_FAMILY}" == "" ]; then
              echo "Need to set MODEL_FAMILY!"
              exit 1
          fi
  
          xinference-local --host 0.0.0.0 --port 8080 &
          PID=$!
          while [ true ];
          do
              curl http://127.0.0.1:8080/docs
              if [ $? -eq 0 ]; then
                  break
              else
                  echo "waiting xinference-local server to become ready..."
                  sleep 1
              fi
          done
          # 1. Check the number of available GPUs to decide whether to enable multi-GPU tensor parallelism.
          GPU_COUNT=$(python3 -c "import torch; print(torch.cuda.device_count())")
          echo "Starting serving model name: ${MODEL_NAME}, num gpus: ${GPU_COUNT}"
          if [ ${GPU_COUNT} -lt 1 ]; then
              echo "No GPUs found. Please check if the container have aquired any GPU device"
              exit 1
          fi
  
          # 2. Start the Xinference model serving.
          set -e
          xinference launch --model_path "${MODEL_PATH}" --model-engine "${MODEL_ENGINE}" -u "${MODEL_UID}" -n "${MODEL_FAMILY}" --n-gpu ${GPU_COUNT} -e http://127.0.0.1:8080 $@
          xinference list -e http://127.0.0.1:8080
          echo "model load succeeded, waiting server process: ${PID}..."
          wait ${PID}
        # Add this line to use $@ in the script:
        # see: https://unix.stackexchange.com/questions/144514/add-arguments-to-bash-c
        - bash
        resources:
          limits:
            cpu: 2
            memory: 6Gi
          requests:
            cpu: 2
            memory: 6Gi
        startupProbe:
          httpGet:
            path: /docs
            port: 8080
            scheme: HTTP
          # The pod will be killed if the model is not available within 10 minutes.
          failureThreshold: 60
          periodSeconds: 10
          timeoutSeconds: 10
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          privileged: false
          runAsNonRoot: true
          runAsUser: 1000
    supportedModelFormats:
      - name: transformers
        version: "1"
  ```
</Tab>
<Tab label="NPU">
  ```yaml
  apiVersion: serving.kserve.io/v1alpha1
  kind: ClusterServingRuntime
  metadata:
    name: aml-xinference-npu
    labels:
      cpaas.io/runtime-class: xinference # required runtime type label
      cpaas.io/accelerator-type: npu
      cpaas.io/cann-version: "7.2"
    annotations:
      cpaas.io/display-name: xinference-npu # Display name in the UI
  spec:
    containers:
      - name: kserve-container
        image: <Replace with your actual Xinference NPU image>
        env:
        - name: MODEL_PATH
          value: /mnt/models/{{ index .Annotations "aml-model-repo" }}
        - name: MODEL_UID
          value: '{{ index .Annotations "aml-model-repo" }}'
        - name: MODEL_ENGINE
          value: "transformers"
        # Required parameter for xinference runtime, please set it based on your model family, value: "llama" # e.g., "llama", "chatglm", etc.
        - name: MODEL_FAMILY
          value: ""
        command:
          - bash
          - -lc
          - |
            set +e
  
            pip install transformers~=4.49.0
  
            if [ "${MODEL_PATH}" == "" ]; then
                echo "Need to set MODEL_PATH!"
                exit 1
            fi
  
            if [ "${MODEL_ENGINE}" == "" ]; then
                echo "Need to set MODEL_ENGINE!"
                exit 1
            fi
  
            if [ "${MODEL_UID}" == "" ]; then
                echo "Need to set MODEL_UID!"
                exit 1
            fi
  
            if [ "${MODEL_FAMILY}" == "" ]; then
                echo "Need to set MODEL_FAMILY!"
                exit 1
            fi
  
  
            xinference-local --host 0.0.0.0 --port 8080 &
  
            PID=$!
  
            while [ true ];
  
            do
                curl http://127.0.0.1:8080/docs
                if [ $? -eq 0 ]; then
                    break
                else
                    echo "waiting xinference-local server to become ready..."
                    sleep 1
                fi
            done
  
            # 1. Check the number of available NPUs to decide whether to enable multi-NPU tensor parallelism.
  
            NPU_COUNT=$(python3 -c "import torch_npu; print(torch_npu.npu.device_count())")
  
            echo "Starting serving model name: ${MODEL_NAME}, num gpus: ${NPU_COUNT}"
  
            if [ ${NPU_COUNT} -lt 1 ]; then
                echo "No NPUs found. Please check if the container have aquired any NPU device"
                exit 1
            fi
  
  
            # 2. Start the Xinference model serving.
  
            set -e
  
            xinference launch --model_path "${MODEL_PATH}" --model-engine "${MODEL_ENGINE}" -u "${MODEL_UID}" -n "${MODEL_FAMILY}" --n-gpu ${GPU_COUNT} -e http://127.0.0.1:8080 $@
  
            xinference list -e http://127.0.0.1:8080
  
            echo "model load succeeded, waiting server process: ${PID}..."
  
            wait ${PID}
          - bash
        resources:
          limits:
            cpu: 2
            memory: 6Gi
          requests:
            cpu: 2
            memory: 6Gi
        startupProbe:
          httpGet:
            path: /docs
            port: 8080
            scheme: HTTP
          # The pod will be killed if the model is not available within 10 minutes.
          failureThreshold: 60
          periodSeconds: 10
          timeoutSeconds: 10
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          privileged: false
          runAsNonRoot: true
          runAsUser: 1000
    supportedModelFormats:
      - name: transformers
        version: "1"    
  ```
</Tab>
<Tab label="CPU">
  ```yaml
  apiVersion: serving.kserve.io/v1alpha1
  kind: ClusterServingRuntime
  metadata:
    name: aml-xinference-cpu
    labels:
      cpaas.io/runtime-class: xinference # required runtime type label
      cpaas.io/accelerator-type: "cpu"
    annotations:
      cpaas.io/display-name: xinference-cpu # Display name in the UI
  spec:
    containers:
      - name: kserve-container
        image: build-harbor.alauda.cn/mlops/xinference:1.2.2-cpu-v1.3.0
        env:
        - name: MODEL_PATH
          value: /mnt/models/{{ index .Annotations "aml-model-repo" }}
        - name: MODEL_UID
          value: '{{ index .Annotations "aml-model-repo" }}'
        - name: MODEL_ENGINE
          value: "transformers"
        # Required parameter for xinference runtime, please set it based on your model family, value: "llama" # e.g., "llama", "chatglm", etc.
        - name: MODEL_FAMILY
          value: ""
        command:
        - bash
        - -lc
        - |
          set +e
          if [ "${MODEL_PATH}" == "" ]; then
              echo "Need to set MODEL_PATH!"
              exit 1
          fi
          if [ "${MODEL_ENGINE}" == "" ]; then
              echo "Need to set MODEL_ENGINE!"
              exit 1
          fi
          if [ "${MODEL_UID}" == "" ]; then
              echo "Need to set MODEL_UID!"
              exit 1
          fi
          if [ "${MODEL_FAMILY}" == "" ]; then
              echo "Need to set MODEL_FAMILY!"
              exit 1
          fi
  
          xinference-local --host 0.0.0.0 --port 8080 &
          PID=$!
          while [ true ];
          do
              curl http://127.0.0.1:8080/docs
              if [ $? -eq 0 ]; then
                  break
              else
                  echo "waiting xinference-local server to become ready..."
                  sleep 1
              fi
          done
  
          
          # Start the Xinference model serving.
          set -e
          xinference launch --model_path "${MODEL_PATH}" --model-engine "${MODEL_ENGINE}" -u "${MODEL_UID}" -n "${MODEL_FAMILY}" -e http://127.0.0.1:8080 $@
          xinference list -e http://127.0.0.1:8080
          echo "model load succeeded, waiting server process: ${PID}..."
          wait ${PID}
        # Add this line to use $@ in the script:
        # see: https://unix.stackexchange.com/questions/144514/add-arguments-to-bash-c
        - bash
        resources:
          limits:
            cpu: 2
            memory: 6Gi
          requests:
            cpu: 2
            memory: 6Gi
        startupProbe:
          httpGet:
            path: /docs
            port: 8080
            scheme: HTTP
          # The pod will be killed if the model is not available within 10 minutes.
          failureThreshold: 60
          periodSeconds: 10
          timeoutSeconds: 10
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          privileged: false
          runAsNonRoot: true
          runAsUser: 1000
    supportedModelFormats:
      - name: transformers
        version: "1"
  ```
</Tab>
</Tabs>
The Xinference runtime requires specific environment variables to function correctly.
    * **Environment Variable Parameter Description**
        | Parameter Name   | Description                                                                                                                                                                                                                                                                                                                      |
        | :--------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
        | `MODEL_FAMILY`   | **Required**. Specifies the family type of the LLM model you are deploying. Xinference uses this parameter to identify and load the correct inference logic for the model. For example, if you are deploying a Llama 3 model, set it to `llama`; if it's a ChatGLM model, set it to `chatglm`. Please set this based on your model's actual family. |

    * **Example**:
        * **Variable Name**: `MODEL_FAMILY`
        * **Variable Value**: `llama` (if you are using a Llama series model)
