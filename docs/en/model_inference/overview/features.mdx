---
weight: 20
---

# Features

## Model Management Functions

- **Function 1: Git-based Model Repository**  
  A complete Git-managed storage solution supporting:
  - **Repository Management**: Create/delete repos with metadata (name/description/visibility) and dependency checks
  - **File Operations**: Web UI upload for small files + CLI/Git LFS for large files (e.g., `*.h5`, `*.bin`)
  - **Version Control**: Full Git capabilities including:
    - Branching (e.g., `main`/`experimental`)
    - Tagging (e.g., `v1.0`)
    - Automatic metadata sync from `README.md`

- **Function 2: Cross-Tenant Sharing**  
  Enables controlled model sharing through:
  - "Shared" visibility setting for inter-tenant access  
  - `public` namespace marketplace for open-source models

- **Function 3: MLOps Integration**  
  Seamless workflow connections:
  - One-click deployment to inference services  
  - Direct notebook access to versioned models
   

## Inference Service Functions

- **Direct Model Deployment for Inference Services**
  * Allows users to directly select specific versions of model files from the model repository and specify the inference runtime image to quickly deploy online inference services. The system automatically downloads, caches, and loads the model, starting the inference service. This simplifies the model deployment process and lowers the deployment threshold.

- **Custom Image Deployment for Inference Services**
  * Supports users in writing Dockerfiles to package models and their dependencies into custom images, and then deploy inference services through standard Kubernetes Deployments. This approach provides greater flexibility, allowing users to customize the inference environment according to their needs.
  
- **Inference Service Experience**
  * Supports batch operations on multiple inference services, such as batch starting, stopping, updating, and deleting.
  * Able to support the creation, monitoring, and result export of batch inference tasks.
  * Provides batch resource management, which can allocate and adjust the resources of inference services in batches.


